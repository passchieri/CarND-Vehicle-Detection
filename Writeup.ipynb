{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Vehicle Detection Project\n",
    "\n",
    "**Author: Igor Passchier**\n",
    "**email: igor.passchier@tassinternational.com**\n",
    "\n",
    "\n",
    "## Goals\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier\n",
    "* Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector. \n",
    "* Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing.\n",
    "* Implement a sliding-window technique and use your trained classifier to search for vehicles in images.\n",
    "* Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n",
    "* Estimate a bounding box for vehicles detected.\n",
    "\n",
    "## References\n",
    "* The code for the project can be found in the jupyter [notebook](Vehicle-Detection.ipynb)\n",
    "* All example images can be found in the [output_images](output_images) folder\n",
    "* Also the final [video](output_images/result.mp4) can be found in this directory\n",
    "\n",
    "[//]: # (Image References)\n",
    "[hogcar]: ./output_images/hogcar.png\n",
    "[hognocar]: ./output_images/hognocar.png\n",
    "[hsvcar]: ./output_images/hsvcar.png\n",
    "[histcar]: ./output_images/histcar.png\n",
    "[scaledcar]: ./output_images/scaledcar.png\n",
    "[incorrectcars]: ./output_images/incorrectcars.png\n",
    "\n",
    "[bbox1]: ./output_images/bbox1.png\n",
    "[bbox2]: ./output_images/bbox2.png\n",
    "[bboxed]: ./output_images/bboxed.png\n",
    "[identified]: ./output_images/identified.png\n",
    "[heatmap]: ./output_images/heatmap.png\n",
    "[final]: ./output_images/final.png\n",
    "\n",
    "[result]: ./output_images/result.mp4\n",
    "\n",
    "## [Rubric](https://review.udacity.com/#!/rubrics/513/view) Points \n",
    "\n",
    "---\n",
    "### Writeup / README\n",
    "\n",
    "#### 1. Provide a Writeup / README that includes all the rubric points and how you addressed each one.  \n",
    "\n",
    "This document. References to the code are made to the sections in the jupyter notebook\n",
    "\n",
    "### Histogram of Oriented Gradients (HOG)\n",
    "\n",
    "#### 1. Explain how (and identify where in your code) you extracted HOG features from the training images.\n",
    "\n",
    "The notebook starts with some imports and helper functions for plotting images. In the second section \"Create training data file names\" I have a generic function to create filenames based on a regexp. Anything with `non-vehicles` in the name is treated as not being a vehicle, while all others are considered cars. By default, I shuffle the data, and the function also allows to limit the number of images (useful for testing further on in the code with a limited set of images.\n",
    "The function `read_imnage` actually reads the file, and applies the required scaling in case of a .png file\n",
    "\n",
    "In most cases, I created seperate blocks with the function definition, followed by a code block to execute the code and to play around with the parameters. In this case, I only print the number of images found, abouth 9000 cars and 9000 non-cars.\n",
    "\n",
    "In the section \"Hog features extraction\" I defined a generic function to call skimage.hog(), and return the feature fector and hog_image.\n",
    "\n",
    "####2. Explain how you settled on your final choice of HOG parameters.\n",
    "\n",
    "I have played around with various values of orientation and pix_per_cell. These have a clear effect on the output hog image. I tried to make them small enough to still see clear features. Increasing them gives more details, but of course also large feature vector, and thus slower feature extraction and vehicle detection with the final pipeline. \n",
    "\n",
    "I settled for orient=9 and pix_per_cell=8 as an acceptable comprimize. Later, during training of the classifier I made small variations around those values to see the effect on the speed and quality of the classifier.\n",
    "\n",
    "I did something similar for the color spaces. But also here, the final decission is only made during training of the classifier.\n",
    "\n",
    "Below are examples for the Y channel of YCrCb color space, with the above mentioned values for the other parameters, for a car and non-car respectively. The hog images are clearly different, which should be good for the classifier.\n",
    "\n",
    "\n",
    "![alt text][hogcar]\n",
    "\n",
    "![alt text][hognocar]\n",
    "\n",
    "\n",
    "#### 3. Describe how (and identify where in your code) you trained a classifier using your selected HOG features (and color features if you used them).\n",
    "\n",
    "Spatial binning is implemented in the \"Spatial binning\" section, while color histogramming is implemented in \"Color histogramming\". I played around with color spaces and looked visually to the differences in the histograms of the features. HSV color space gave the most pronounced differences, so that is what I decided to use.\n",
    "Below are the test images for spatial binning and color histogramming, respectively.\n",
    "\n",
    "\n",
    "![alt text][hsvcar]\n",
    "\n",
    "![alt text][histcar]\n",
    "\n",
    "In the final feature set I used for the training, I decided to keep all three types of features: hog, spatial, and color.\n",
    "\n",
    "In the section \"Feature extraction\" I implement the functions to extract the features from one or more images. `single_img_features` extracts the features of a single image, with all relevant parameters of the algorithm as function parameters. The defaults in the code are the values I finally used.\n",
    "\n",
    "The function `extract_features` reads all images based the file names and extracts and returns all features.\n",
    "\n",
    "In the second code block, all features of all training data are determined. In line 10, also the scaler of the features is trained. This is clearly necessay, because the values for the different features fluctuates heavenly, see below.\n",
    "\n",
    "![alt text][scaledcar]\n",
    "\n",
    "In the section \"Create training data\", all steps are brought together:\n",
    "\n",
    "* line 1-19: set all parameters\n",
    "* line 26-47: extract scaled features\n",
    "* line 51-53: splt the data in training and test data\n",
    "* line 56-61: provide some statistics over the whole process\n",
    "\n",
    "In the section \"Train the model\" I have implemented the training functions. I have started with a linear SVC model and used to investigate further the effects of hog parameters and color spaces. Afterwards, when I settled on the feature extraction parameters, I used GridSearchCV with SVC to optimize the parameters. As the parameter space is very large, I started by variying a single parameter in big steps, then with smaller steps around the optimium. Finally, a used a full parameter search for C and gamma in 3 steps each. Initially, I experimented with kernels `rbf` and `linear`, but found that the `rbf` kernel give better results. For the C parameter, I searched between 1-1000, and for gamma between 0.1-10 * 1/2000 (2000 is about the number of features in my feature vector. 1/number of features is the default, so therefore I searched around that value.\n",
    "\n",
    "I got the best results for \n",
    "* kernel = rbf\n",
    "* C=10\n",
    "* gamma=0.0005\n",
    "\n",
    "Resulting in a test accuracy of 0.9977\n",
    "\n",
    "In the next section, I created a classifier function `classify_image`, taking an input image and returning the classification prediction.\n",
    "\n",
    "In the section \"Inspect incorrectly classified test images\", I have inspected the images that remained incorrectly classified. Especially for the incorrectly classified cars it is clear that this are mainly car pictures taken from the rear-left. To fix this, a larger training set with these type of images are required.\n",
    "\n",
    "![alt text][incorrectcars]\n",
    "\n",
    "\n",
    "\n",
    "### Sliding Window Search\n",
    "\n",
    "#### 1. Describe how (and identify where in your code) you implemented a sliding window search.  How did you decide what scales to search and how much to overlap windows?\n",
    "\n",
    "The sliding window search is split in several function. I have made to versions: first an approach where every window is treated as an individual image, later a second set of functions where the hog feature extraction is mixed with the sliding window search to optimnize the pipeline.\n",
    "\n",
    "`slide_window` generated windows of a specific size (`xy_window`) for a specific region of an image (image size is `shape`, region is defined by `x_start_stop` and `y_start_stop`. Also the overlap can be specified.\n",
    "\n",
    "A second function `create_winows` uses this function to generate the windows in different sizes.\n",
    "\n",
    "In the next section, I implementeda utility function `draw_boxes` to plot bounding boxes on top of an image in specific colors. This is used in the next section to investigate the windows to use. Below are individual images for the three sizes of windows (overlap set to 0, to clearly see the size), and the complete set of windows (here, the overlap has been set to the real value of 0.75.\n",
    "\n",
    "![][bbox1]\n",
    "\n",
    "![][bbox2]\n",
    "\n",
    "For tuning the window sizes, I have created 6 additional snapshots from the training video, with cars at different distances. I also did some tuning by running the full pipelines with different scales and overlaps.\n",
    "\n",
    "In \"Perform the search in all defined windows\" a function `search_windows` is implemented, looping over all windows, running the classifier, and storing the identified windows.\n",
    "\n",
    "The `search_one` function is wrapper, where all parameters are taken from global parameters, which makes it easier to use in the pipeline. Later, I replaced this function in the pipeline by the optimized version where hog_feature extraction and sliding window is mixed.\n",
    "\n",
    "Below is an example of the identified windows on a sample image\n",
    "\n",
    "![][bboxed]\n",
    "\n",
    "Next step is to combine the pixels in all hot windows, apply a threshold, and generate labels for the resulting areas. This is done in section \"Heat mapping and labels\". The images below show the heatmap, labels, and final boundingbox of the identied cars on a sample image.\n",
    "\n",
    "![][heatmap]\n",
    "\n",
    "![][identified]\n",
    "\n",
    "\n",
    "\n",
    "#### 2. Show some examples of test images to demonstrate how your pipeline is working.  What did you do to optimize the performance of your classifier?\n",
    "\n",
    "In \"Optimized function to determine hot windows\" I have implemented a function `find_cars` This function scans the image for all windows of a specific scale, by combining feature extraction and classification.\n",
    "\n",
    "In line 12-23 the relevant area of the image is selected, scaled and color converted.\n",
    "In line 38-40 the hog features for the complete area are determined.\n",
    "In line 42-66, the image area is scanned with the window, features are extracted, and classification is performed.\n",
    "\n",
    "In line 67, I draw the result on the image. I trick the system by setting the `all` parameter to `True`, which provides me an image overlayed with all the windows being scanned. This is useful for debugging, not for the final result.\n",
    "\n",
    "The `search_one_fast` function uses the find_cars function at different scales and different areas, and combines the hot windows. The function allows to use 4 different windows size, which can be turned on and off via changing the (hard_coded) True and False values (e.g. in line 8, 17, etc.). The `search_one_fast` function is a drop in replacement of the `search_one` function defined earlier, allowing easy switching in the pipeliine between the two.\n",
    "\n",
    "Development performance was optimized by making a save and restore function for the classifier and global parameter definition, see section \"Functions to save and restore the full state\". These are based on `pickle.dump` and `pickle.load`\n",
    "\n",
    "Then, in \"Pipeline definition\" I implement an initialization function, to (re)set all global parameters used, and a pipeline function.\n",
    "\n",
    "The pipeline function contains 2 optimizations:\n",
    "1. To optimize tuning speed, a parameter `ratio` is used. This is the ratio between number of frames in the video, and number of frames analysed. So if ratio=1, all frames are analysed, but if ratio=25, only 1 frame per second (25 Hz video) is analysed. In that way, I can go through the complete video and sample only a subset of the images. This greatly speeds up tuning times\n",
    "2. The second optimization is to reduce false positives, and to stabalize the detected bounding boxes. The global parameter `roling` determines over how many heatmaps from consequetive frames an integrated heatmap is generated. A threshold `roling_threshold` on the integrated heatmap is specified, similar to the threshold on the heatmap of the individual frames.\n",
    "\n",
    "The roling average works best if the normal threshold on the heatmap is put to 0 (so no threshold on the individual heatmaps), and a threshold slightly larger than the number of frames averaged in the roling average. In this way, an area needs to be identified in more than one window in at least one frame, and should be identified in the majority of frames at least once. \n",
    "\n",
    "If the roling average is done over too many frames, the bounding box starts lagging behind. Therefore, I fixed the roling average over 5 frames, and a threshold of 6.\n",
    "\n",
    "A example from the final pipeline is shown below\n",
    "\n",
    "\n",
    "![][final]\n",
    "\n",
    "### Video Implementation\n",
    "\n",
    "#### 1. Provide a link to your final video output.  Your pipeline should perform reasonably well on the entire project video (somewhat wobbly or unstable bounding boxes are ok as long as you are identifying the vehicles most of the time with minimal false positives.)\n",
    "Here's the resulting video [result]\n",
    "\n",
    "#### 2. Describe how (and identify where in your code) you implemented some kind of filter for false positives and some method for combining overlapping bounding boxes.\n",
    "\n",
    "The heatmapping, labelling, and roling average have been described before.\n",
    "\n",
    "\n",
    "### Discussion\n",
    "\n",
    "#### 1. Briefly discuss any problems / issues you faced in your implementation of this project.  Where will your pipeline likely fail?  What could you do to make it more robust?\n",
    "\n",
    "I found the performance optimization the most difficult part. So may parameters are available, like \n",
    "\n",
    "* what features to use, and how many of each\n",
    "* what classifier to use, and with what parameters. \n",
    "* what size and what locations to use for the sliding window\n",
    "\n",
    "And they all work together, e.g. maybe use less features, but more windows, or the other way around. \n",
    "\n",
    "If a vehicle has been identified, this could be used in the next step to only search in that area in the next frame. That could speed up significantly, and also reduce false positives. New vehicles could be searched for then only in the (lower) left and right side of the frame (and then they should be large), or at the horizon (but then they should be small). I did not implement this, as it would require significant tuning, especially if you want to make it stable also for other videos.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
